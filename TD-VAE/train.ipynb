{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import MovingMNIST, MovingMNISTLR\n",
    "from model import TDVAE\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps=2*10**4\n",
    "batch_size=32\n",
    "dataset_type='MovingMNISTLR'\n",
    "root_log_dir='log/'\n",
    "data_dir='../data/MNIST/'\n",
    "log_dir=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "log_interval=200\n",
    "save_interval=1000\n",
    "workers=0\n",
    "seed=1234\n",
    "device_ids=[0]\n",
    "z_size=8\n",
    "lr=5e-4\n",
    "rescale=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions (for training): \n",
      "  p_{b}(z_{t1}|b_{t1}), p_{b}(z_{t2}|b_{t2}), p_{t}(z_{t2}|z_{t1}), p_{d}(x_{t2}|z_{t2}), q(z_{t1}|z_{t2},b_{t1},b_{t2}), p(b|x) \n",
      "Loss function: \n",
      "  mean \\left(\\mathbb{E}_{p(b|x)} \\left[\\sum_{t=1}^{19} \\mathbb{E}_{f(x_{t2},b_{t1},b_{t2}|t,x,b)} \\left[\\mathbb{E}_{p_{b}(z_{t2}|b_{t2})} \\left[D_{KL} \\left[q(z_{t1}|z_{t2},b_{t1},b_{t2})||p_{b}(z_{t1}|b_{t1}) \\right] + \\mathbb{E}_{q(z_{t1}|z_{t2},b_{t1},b_{t2})} \\left[\\log p_{b}(z_{t2}|b_{t2}) - \\log p_{d}(x_{t2}|z_{t2}) - \\log p_{t}(z_{t2}|z_{t1}) \\right] \\right] \\right] \\right] \\right) \n",
      "Optimizer: \n",
      "  Adam (\n",
      "  Parameter Group 0\n",
      "      amsgrad: False\n",
      "      betas: (0.9, 0.999)\n",
      "      eps: 1e-08\n",
      "      lr: 0.0005\n",
      "      weight_decay: 0\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = f\"cuda:{device_ids[0]}\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Seed\n",
    "if seed!=None:\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "# Logging\n",
    "log_interval_num = log_interval\n",
    "log_dir = os.path.join(root_log_dir, log_dir)\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(os.path.join(log_dir, 'models'))\n",
    "os.mkdir(os.path.join(log_dir,'runs'))\n",
    "writer = SummaryWriter(log_dir=os.path.join(log_dir,'runs'))\n",
    "\n",
    "# Dataset\n",
    "if dataset_type == 'MovingMNIST':\n",
    "    data_path = os.path.join(data_dir, 'mnist_test_seq.npy')\n",
    "    full_dataset = MovingMNIST(data_path, rescale=rescale)\n",
    "    data_num = len(full_dataset)\n",
    "    train_size = int(0.9 * data_num)\n",
    "    test_size = data_num - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "elif dataset_type == 'MovingMNISTLR':\n",
    "    train_dataset = MovingMNISTLR(data_dir, train=True, download=True)\n",
    "    test_dataset = MovingMNISTLR(data_dir, train=False, download=True)\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "train_loader_iterator = iter(train_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader_iterator = iter(test_loader)\n",
    "test_batch = next(test_loader_iterator).to(device)\n",
    "test_batch = test_batch.transpose(0, 1)\n",
    "seq_len, _, C, H, W = test_batch.size()\n",
    "\n",
    "model = TDVAE(seq_len=seq_len, z_size=z_size, x_size=C*H*W, processed_x_size=C*H*W,\n",
    "              optimizer=torch.optim.Adam, optimizer_params={\"lr\": lr}, device=device, clip_grad_value=10)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(\\mathbb{E}_{p(b|x)} \\left[\\sum_{t=1}^{19} \\mathbb{E}_{f(x_{t2},b_{t1},b_{t2}|t,x,b)} \\left[\\mathbb{E}_{p_{b}(z_{t2}|b_{t2})} \\left[D_{KL} \\left[q(z_{t1}|z_{t2},b_{t1},b_{t2})||p_{b}(z_{t1}|b_{t1}) \\right] + \\mathbb{E}_{q(z_{t1}|z_{t2},b_{t1},b_{t2})} \\left[\\log p_{b}(z_{t2}|b_{t2}) - \\log p_{d}(x_{t2}|z_{t2}) - \\log p_{t}(z_{t2}|z_{t1}) \\right] \\right] \\right] \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr in tqdm(range(gradient_steps)):\n",
    "    try:\n",
    "        batch = next(train_loader_iterator)\n",
    "    except StopIteration:\n",
    "        train_loader_iterator = iter(train_loader)\n",
    "        batch = next(train_loader_iterator)\n",
    "    batch = batch.to(device)\n",
    "    batch_size, seq_len, *_ = batch.size()\n",
    "    batch = batch.view(batch_size, seq_len, -1)\n",
    "    batch = batch.transpose(0, 1)\n",
    "\n",
    "    loss = model.train({\"x\": batch})\n",
    "    writer.add_scalar('train_loss', loss, itr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if itr % log_interval_num == 0:\n",
    "            test_pred = model.pred(test_batch)\n",
    "            test_loss = model.test({\"x\": batch.view(seq_len, batch_size, -1)})\n",
    "\n",
    "            writer.add_scalar('test_loss', test_loss, itr)\n",
    "            writer.add_video('test_pred', test_pred.transpose(0, 1), itr)\n",
    "            writer.add_video('test_ground_truth', test_batch.transpose(0, 1), itr)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
