{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent Generative Query Networks\n",
    "### https://deepmind.com/research/publications/consistent-generative-query-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no open MNIST Dice datasets from DeepMind, but you can make them by using this.\n",
    "##### https://github.com/musyoku/gqn-dataset-renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also train CGQN using GQN datasets like Shepard-Metzler.\n",
    "##### Datasets: https://github.com/deepmind/gqn-datasets\n",
    "##### Datasets Translater: https://github.com/l3robot/gqn_datasets_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f214406b0d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pixyz.distributions import Normal\n",
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shepardmetzler import ShepardMetzler, Scene, transform_viewpoint\n",
    "from conv_lstm import Conv2dLSTMCell\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Representation(nn.Module):\n",
    "    def __init__(self, nf_v, nf_f):\n",
    "        super(Representation, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nf_v+nf_f, 8, kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, v, f):\n",
    "        # Increase dimensions\n",
    "        v = v.view(v.size(0), -1, 1, 1)\n",
    "        v = v.repeat(1, 1, f.size(2), f.size(3))\n",
    "        \n",
    "        h = F.relu(self.conv1(torch.cat([v, f], dim=1)))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        r = self.conv4(h)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posterior(Normal):\n",
    "    def __init__(self, nf_to_hidden=64, nf_z=3):\n",
    "        super(Posterior, self).__init__(cond_var=[\"h_e\"], var=[\"z\"])\n",
    "        self.nf_z = nf_z\n",
    "        self.conv = nn.Conv2d(nf_to_hidden, 2*nf_z, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, h_e):\n",
    "        mu, logvar = torch.split(self.conv(h_e), self.nf_z, dim=1)\n",
    "        return {\"loc\": mu, \"scale\": F.softplus(logvar)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(Normal):\n",
    "    def __init__(self, nf_to_obs=128, nf_z=3):\n",
    "        super(Prior, self).__init__(cond_var=[\"h_d\"], var=[\"z\"])\n",
    "        self.nf_z = nf_z\n",
    "        self.conv = nn.Conv2d(nf_to_obs, 2*nf_z, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, h_d):\n",
    "        mu, logvar = torch.split(self.conv(h_d), self.nf_z, dim=1)\n",
    "        return {\"loc\": mu, \"scale\": F.softplus(logvar)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Renderer(Normal):\n",
    "    def __init__(self, nf_to_obs=128, nf_f=3):\n",
    "        super(Renderer, self).__init__(cond_var=[\"h_d\", \"var\"], var=[\"f\"])\n",
    "        self.convt = nn.ConvTranspose2d(nf_to_obs, nf_f, kernel_size=2, stride=2)\n",
    "    def forward(self, h_d, var):\n",
    "        mu = self.convt(h_d)\n",
    "        return {\"loc\": mu, \"scale\": math.sqrt(var)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGQN(nn.Module):\n",
    "    def __init__(self, nf_v=7, nf_f=3, nf_r=32, nt=4, nf_to_hidden=64, nf_enc=128, nf_to_obs=128, nf_dec=64, nf_z=3):\n",
    "        super(CGQN, self).__init__()\n",
    "        self.nf_f = nf_f\n",
    "        self.nf_to_hidden = nf_to_hidden\n",
    "        self.nf_to_obs = nf_to_obs\n",
    "        self.nt = nt\n",
    "        \n",
    "        self.m_theta = Representation(nf_v, nf_f)\n",
    "        \n",
    "        self.encoder = nn.Conv2d(nf_f, nf_enc, kernel_size=2, stride=2)\n",
    "        self.decoder = nn.Conv2d(nf_f, nf_dec, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.m_gamma = Renderer(nf_to_obs, nf_f)\n",
    "        \n",
    "\n",
    "        # Outputs parameters of distributions\n",
    "        self.posterior = Posterior(nf_to_hidden, nf_z)\n",
    "        self.prior = Prior(nf_to_obs, nf_z)\n",
    "\n",
    "        # Recurrent encoder/decoder models\n",
    "        self.lstm_enc = Conv2dLSTMCell(nf_enc+nf_r+nf_to_obs, nf_to_hidden, kernel_size=5, stride=1, padding=2)\n",
    "        self.lstm_dec = Conv2dLSTMCell(nf_z+nf_dec+nf_r, nf_to_obs, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        self.upsample   = nn.ConvTranspose2d(nf_r, nf_r, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, v, f, v_prime, f_T, var):\n",
    "        batch_size, m, _, h, w = f.size()\n",
    "        # num of target\n",
    "        k = f_T.size(1)\n",
    "        # merge batch and view dimensions.\n",
    "        _, _, *v_dims = v.size()\n",
    "        _, _, *f_dims = f.size()\n",
    "\n",
    "        v = v.view((-1, *v_dims))\n",
    "        f = f.view((-1, *f_dims))\n",
    "        \n",
    "        v_prime = v_prime.view((-1, *v_dims))\n",
    "        f_T = f_T.view((-1, *f_dims))\n",
    "        \n",
    "        r = self.m_theta(v, f)\n",
    "        r_T = self.m_theta(v_prime, f_T)\n",
    "        \n",
    "        # seperate batch and view dimensions\n",
    "        _, *r_dims = r.size()\n",
    "        r = r.view((batch_size, m, *r_dims))\n",
    "        r_T = r_T.view((batch_size, k, *r_dims))\n",
    "\n",
    "        # sum over view representations\n",
    "        r = torch.sum(r, dim=1)\n",
    "        r_T = torch.sum(r_T, dim=1)\n",
    "        \n",
    "        # expand dimensions\n",
    "        r = r.repeat(1, k, 1, 1)\n",
    "        r = r.view((-1, *r_dims))\n",
    "        r_T = r_T.repeat(1, k, 1, 1)\n",
    "        r_T = r_T.view((-1, *r_dims))\n",
    "\n",
    "        # hidden states\n",
    "        h_e = f.new_zeros((batch_size*k, self.nf_to_hidden, h//2, w//2))\n",
    "        h_d = f.new_zeros((batch_size*k, self.nf_to_obs, h//2, w//2))\n",
    "\n",
    "        # cell states\n",
    "        c_e = f.new_zeros((batch_size*k, self.nf_to_hidden, h//2, w//2))\n",
    "        c_d = f.new_zeros((batch_size*k, self.nf_to_obs, h//2, w//2))\n",
    "\n",
    "        canvas = f.new_zeros((batch_size*k, self.nf_f, h, w))\n",
    "        r = self.upsample(r)\n",
    "        r_T = self.upsample(r_T)\n",
    "        enc_input = self.encoder(f_T)\n",
    "        \n",
    "        kl = 0\n",
    "        for _ in range(self.nt):\n",
    "            \n",
    "            # update encoder LSTM states\n",
    "            h_e, c_e = self.lstm_enc(torch.cat([enc_input, r_T, h_d], dim=1), [h_e, c_e])\n",
    "\n",
    "            # sample from posterior\n",
    "            z = self.posterior.sample({\"h_e\": h_e}, reparam=True)[\"z\"]\n",
    "            \n",
    "            # kl divergence between posterior and prior\n",
    "            _kl = KullbackLeibler(self.posterior, self.prior).mean()\n",
    "            _kl = _kl.estimate({\"h_e\": h_e, \"h_d\": h_d})\n",
    "            kl += _kl\n",
    "\n",
    "            dec_input = self.decoder(canvas)\n",
    "\n",
    "            # update decoder LSTM states\n",
    "            h_d, c_d = self.lstm_dec(torch.cat([z, dec_input, r], dim=1), [h_d, c_d])\n",
    "\n",
    "            # refine representation\n",
    "            canvas = self.m_gamma.sample_mean({\"h_d\": h_d, \"var\": var})\n",
    "            \n",
    "        f_R = torch.clamp(canvas.view((batch_size, k, *f_dims)), 0, 1)\n",
    "        f_R_noise = self.m_gamma.sample({\"h_d\": h_d, \"var\": var}, reparam=True)[\"f\"]\n",
    "        MSE = nn.MSELoss()\n",
    "        mse = MSE(f_T, f_R_noise)\n",
    "\n",
    "        return f_R, mse, kl\n",
    "    \n",
    "    def sample(self, v, f, v_prime):\n",
    "        batch_size, m, _, h, w = f.size()\n",
    "        \n",
    "        # num of target\n",
    "        k = v_prime.size(1)\n",
    "        \n",
    "        # merge batch and view dimensions.\n",
    "        _, _, *v_dims = v.size()\n",
    "        _, _, *f_dims = f.size()\n",
    "\n",
    "        v = v.view((-1, *v_dims))\n",
    "        f = f.view((-1, *f_dims))\n",
    "        \n",
    "        r = self.m_theta(v, f)\n",
    "        \n",
    "        # seperate batch and view dimensions\n",
    "        _, *r_dims = r.size()\n",
    "        r = r.view((batch_size, m, *r_dims))\n",
    "\n",
    "        # sum over view representations\n",
    "        r = torch.sum(r, dim=1)\n",
    "        \n",
    "        # expand dimensions\n",
    "        r = r.repeat(1, k, 1, 1)\n",
    "        r = r.view((-1, *r_dims))\n",
    "\n",
    "        # hidden states\n",
    "        h_d = f.new_zeros((batch_size*k, self.nf_to_obs, h//2, w//2))\n",
    "        # cell states\n",
    "        c_d = f.new_zeros((batch_size*k, self.nf_to_obs, h//2, w//2))\n",
    "\n",
    "        canvas = f.new_zeros((batch_size*k, self.nf_f, h, w))\n",
    "        r = self.upsample(r)\n",
    "\n",
    "        for _ in range(self.nt):\n",
    "            # sample from prior\n",
    "            z = self.prior.sample({\"h_d\": h_d})[\"z\"]\n",
    "            \n",
    "            dec_input = self.decoder(canvas)\n",
    "            # update decoder LSTM states\n",
    "            h_d, c_d = self.lstm_dec(torch.cat([z, dec_input, r], dim=1), [h_d, c_d])\n",
    "\n",
    "            canvas = self.m_gamma.sample_mean({\"h_d\": h_d, \"var\": 0})\n",
    "            \n",
    "        f_R = torch.clamp(canvas.view((batch_size, k, *f_dims)), 0, 1)\n",
    "\n",
    "        return f_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(v_data, f_data, seed=None):\n",
    "    random.seed(seed)\n",
    "    batch_size, n, *_ = f_data.size()\n",
    "\n",
    "    # Sample random number of views\n",
    "    m = random.randint(1, n-2)\n",
    "#     k = random.randint(m+1, n)\n",
    "    k = m+2\n",
    "\n",
    "    indices = torch.randperm(n)\n",
    "    input_idx, target_idx = indices[:m], indices[m:k]\n",
    "\n",
    "    v, f = v_data[:, input_idx], f_data[:, input_idx]\n",
    "    v_prime, f_T = v_data[:, target_idx], f_data[:, target_idx]\n",
    "    \n",
    "    return v, f, v_prime, f_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22476 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:58: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  4%|â–         | 906/22476 [17:47<7:09:01,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# args\n",
    "train_data_dir = '/workspace/dataset/shepard_metzler_7_parts-torch/train'\n",
    "test_data_dir = '/workspace/dataset/shepard_metzler_7_parts-torch/test'\n",
    "\n",
    "# number of workers to load data\n",
    "num_workers = 0\n",
    "\n",
    "# for logging\n",
    "log_interval_num = 100\n",
    "save_interval_num = 100000\n",
    "dir_name = str(datetime.datetime.now())\n",
    "log_dir = '/workspace/logs/'+ dir_name\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(log_dir+'/models')\n",
    "os.mkdir(log_dir+'/runs')\n",
    "\n",
    "# tensorboardX\n",
    "writer = SummaryWriter(log_dir=log_dir+'/runs')\n",
    "\n",
    "batch_size = 36\n",
    "gradient_steps = 2*(10**6)\n",
    "\n",
    "train_dataset = ShepardMetzler(root_dir=train_data_dir, target_transform=transform_viewpoint)\n",
    "test_dataset = ShepardMetzler(root_dir=test_data_dir, target_transform=transform_viewpoint)\n",
    "\n",
    "\n",
    "# hyperparameters for traveling salesman dataset\n",
    "# nf_v=7\n",
    "# nf_f=3\n",
    "# nf_r=32\n",
    "# nt=4\n",
    "# nf_to_hidden=64\n",
    "# nf_enc=128\n",
    "# nf_to_obs=128\n",
    "# nf_dec=64\n",
    "# nf_z=3\n",
    "# alpha, beta = 2.0, 0.5\n",
    "\n",
    "# var = alpha\n",
    "\n",
    "# hyperparameters for MNIST Cube 3D scene reconstruction task\n",
    "nf_v=7\n",
    "nf_f=3\n",
    "nf_r=32\n",
    "nt=6\n",
    "nf_to_hidden=128\n",
    "nf_enc=128\n",
    "nf_to_obs=128\n",
    "nf_dec=128\n",
    "nf_z=3\n",
    "\n",
    "var = 2.0\n",
    "\n",
    "hyperparam = (nf_v, nf_f, nf_r, nt, nf_to_hidden, nf_enc, nf_to_obs, nf_dec, nf_z)\n",
    "\n",
    "# model\n",
    "model = CGQN(*hyperparam).to(device)\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "kwargs = {'num_workers':num_workers, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "f_data_test, v_data_test = next(iter(test_loader))\n",
    "\n",
    "# number of gradient steps\n",
    "s = 0\n",
    "while True:\n",
    "    for f_data, v_data in tqdm(train_loader):\n",
    "        f_data = f_data.to(device)\n",
    "        v_data = v_data.to(device)\n",
    "        v, f, v_prime, f_T = arrange_data(v_data, f_data)\n",
    "        f_R, mse, kl = model(v, f, v_prime, f_T, var)\n",
    "        mse = mse.mean()\n",
    "        kl = kl.mean()\n",
    "        loss = mse / var + kl\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        writer.add_scalar('train_mse', mse, s)\n",
    "        writer.add_scalar('train_kl', kl, s)\n",
    "        writer.add_scalar('train_loss', loss, s)\n",
    "        \n",
    "        s += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # keep a checkpoint every n steps\n",
    "            if s % log_interval_num == 0 or s == 1:\n",
    "                writer.add_image('train_ground_truth', f_T[0], s)\n",
    "                writer.add_image('train_reconstruction', f_R[0], s)\n",
    "                \n",
    "                f_data_test = f_data_test.to(device)\n",
    "                v_data_test = v_data_test.to(device)\n",
    "                \n",
    "                v_test, f_test, v_prime_test, f_T_test = arrange_data(v_data_test, f_data_test, seed=0)\n",
    "                f_R_test, mse_test, kl_test = model(v_test, f_test, v_prime_test, f_T_test, var)\n",
    "                f_gen_test = model.module.sample(v_test, f_test, v_prime_test)\n",
    "                \n",
    "                mse_test = mse_test.mean()\n",
    "                kl_test = kl_test.mean()\n",
    "                loss_test = mse_test / var + kl_test\n",
    "                \n",
    "                writer.add_scalar('test_mse', mse_test, s)\n",
    "                writer.add_scalar('test_kl', kl_test, s)\n",
    "                writer.add_scalar('test_loss', loss_test, s)\n",
    "                writer.add_image('test_ground_truth', f_T_test[0], s)\n",
    "                writer.add_image('test_reconstruction', f_R_test[0], s)\n",
    "                writer.add_image('test_generation', f_gen_test[0], s)\n",
    "                \n",
    "            if s % save_interval_num == 0:\n",
    "                torch.save(model.state_dict(), log_dir + \"/models/model-{}.pt\".format(s))\n",
    "                \n",
    "            if s >= gradient_steps:\n",
    "                break\n",
    "\n",
    "            # pixel variance for traveling salesman dataset\n",
    "            # var = max(alpha - (alpha - beta)*(s/10**5), beta)\n",
    "            \n",
    "            # pixel variance for MNIST Cube 3D scene reconstruction task\n",
    "            if s >= 100000 and s < 150000:\n",
    "                var = 0.2\n",
    "            elif s >= 150000 and s < 200000:\n",
    "                var = 0.4\n",
    "            elif s >= 200000:\n",
    "                var = 0.9\n",
    "        \n",
    "    if s >= gradient_steps:\n",
    "        torch.save(model.state_dict(), log_dir + \"/models/model-final.pt\")\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
