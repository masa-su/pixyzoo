{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VRNN\n",
    "Original paper: A Recurrent Latent Variable Model for Sequential Data (https://arxiv.org/pdf/1506.02216.pdf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "from pixyz.losses import KullbackLeibler, StochasticReconstructionLoss\n",
    "from pixyz.losses import IterativeLoss\n",
    "from pixyz.distributions import Bernoulli, Normal, Deterministic\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 28\n",
    "h_dim = 100\n",
    "z_dim = 64\n",
    "t_max = x_dim\n",
    "\n",
    "# xのfeature_extraction, Encoderの入力となる\n",
    "class Phi_x(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Phi_x, self).__init__()\n",
    "        self.fc0 = nn.Linear(x_dim, h_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.fc0(x))\n",
    "\n",
    "# 潜在変数zを加工するNN, decoderの入力となる, feature_extraction\n",
    "class Phi_z(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Phi_z, self).__init__()\n",
    "        self.fc0 = nn.Linear(z_dim, h_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return F.relu(self.fc0(z))\n",
    "\n",
    "f_phi_x = Phi_x().to(device)\n",
    "f_phi_z = Phi_z().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zとh_t-1に条件づけられたxt\n",
    "# 入力はfeature_extractされたz, h_t-1\n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\", \"h_prev\"], var=[\"x\"])\n",
    "        self.fc1 = nn.Linear(h_dim + h_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, x_dim)\n",
    "        # MNISTだから?\n",
    "        # 論文上のやつの実装\n",
    "        # self.fc31 = nn.Linear(h_dim, x_dim)\n",
    "        # self.fc32 = nn.Linear(h_dim, x_dim)\n",
    "        self.f_phi_z = f_phi_z\n",
    "\n",
    "    def forward(self, z, h_prev):\n",
    "        h = torch.cat((self.f_phi_z(z), h_prev), dim=-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        # MNISTだから?\n",
    "        # 論文上のやつの実装\n",
    "        # {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "\n",
    "\n",
    "# 潜在変数zのprior, 通常のVAEと違いh_prevにより平均と分散が決まる, 標準正規分布ではない\n",
    "class Prior(Normal):\n",
    "    def __init__(self):\n",
    "        super(Prior, self).__init__(cond_var=[\"h_prev\"], var=[\"z\"])\n",
    "        self.fc1 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc21 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "    def forward(self, h_prev):\n",
    "        h = F.relu(self.fc1(h_prev))\n",
    "        return {\"loc\": self.fc21(h), \"scale\": F.softplus(self.fc22(h))}\n",
    "\n",
    "\n",
    "# 事後分布の推論\n",
    "# feature_extractされたxtと, h_prevによる\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\", \"h_prev\"], var=[\"z\"], name=\"q\")\n",
    "        self.fc1 = nn.Linear(h_dim + h_dim, h_dim)\n",
    "        self.fc21 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "        self.f_phi_x = f_phi_x\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        h = torch.cat((self.f_phi_x(x), h_prev), dim=-1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        return {\"loc\": self.fc21(h), \"scale\": F.softplus(self.fc22(h))}\n",
    "\n",
    "\n",
    "# RNNの部分, x, z, h_prevを入力として次の隠れ状態を出力する\n",
    "class Recurrence(Deterministic):\n",
    "    def __init__(self):\n",
    "        super(Recurrence, self).__init__(cond_var=[\"x\", \"z\", \"h_prev\"], var=[\"h\"])\n",
    "        # 1 層のGRUCell\n",
    "        self.rnncell = nn.GRUCell(h_dim * 2, h_dim).to(device)\n",
    "        # 隠れ状態\n",
    "        self.hidden_size = self.rnncell.hidden_size\n",
    "        \n",
    "        # xtのfeature_extractor\n",
    "        self.f_phi_x = f_phi_x\n",
    "        #zのfeature_extractor\n",
    "        self.f_phi_z = f_phi_z\n",
    "\n",
    "    def forward(self, x, z, h_prev):\n",
    "        extracted_x = self.f_phi_x(x)\n",
    "        extracted_z = self.f_phi_z(z)\n",
    "        \n",
    "        rnn_input_t = torch.cat((extracted_z, extracted_x), dim=-1)\n",
    "        h_next = self.rnncell(rnn_input_t, h_prev)\n",
    "        return {\"h\": h_next}\n",
    "\n",
    "prior = Prior().to(device)\n",
    "decoder = Generator().to(device)\n",
    "encoder = Inference().to(device)\n",
    "recurrence = Recurrence().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_with_recurrence = encoder * recurrence\n",
    "generate_from_prior = prior * decoder * recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(h,z|x,h_{prev}) = p(h|x,z,h_{prev})q(z|x,h_{prev})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_latex(encoder_with_recurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(h,x,z|h_{prev}) = p(h|x,z,h_{prev})p(x|z,h_{prev})p(z|h_{prev})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_latex(generate_from_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda data: data[0])\n",
    "    ])\n",
    "masa_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=mnist_transform),\n",
    "    batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def KLGaussianGaussian(phi_mu, phi_sigma, prior_mu, prior_sigma):\n",
    "    '''\n",
    "    Re-parameterized formula for KL\n",
    "    between Gaussian predicted by encoder and Gaussian dist\n",
    "    '''\n",
    "    kl = 0.5 * (2 * torch.log(prior_sigma) - 2 * torch.log(phi_sigma) + (phi_sigma**2 + (phi_mu - prior_mu2)**2) / prior_sigma**2 - 1)\n",
    "    kl = torch.sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "def Gaussian_nll(y, mu, sigma):\n",
    "    '''\n",
    "    gaussian negative log-likelihood\n",
    "    '''\n",
    "    nll = torch.sum(torch.sqrt(y - mu) / sigma**2 + 2 * torch.log(sigma) + torch.log(2 * math.pi))\n",
    "    nll = 0.5 * nll\n",
    "    return nll\n",
    "\n",
    "\n",
    "def bi_nll(y_hat, y):\n",
    "    '''\n",
    "    binary cross entropy\n",
    "    '''\n",
    "    nll = - (y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))\n",
    "    nll = torch.sum(nll)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VRNN, self).__init__()\n",
    "        self.z_prior = prior\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.rnn = recurrence\n",
    "        self.n_layers = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_dec_mean, all_dec_std = [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "        x_ts = []\n",
    "        dec_ts = []\n",
    "        \n",
    "        # torch.zeros(batch_size, h_dim), 隠れ状態の初期化\n",
    "        h = torch.zeros(x.size(1), h_dim)\n",
    "        \n",
    "        # timestep t分処理を行う(x.size(0)=行数)\n",
    "        for t in range(x.size(0)):\n",
    "            # Encoding\n",
    "            enc_t = self.encoder(x[t], h)\n",
    "            enc_mean_t, enc_std_t = enc_t['loc'], enc_t['scale']\n",
    "            \n",
    "            # prior\n",
    "            prior_t = self.z_prior(h)\n",
    "            prior_mean_t, prior_std_t = prior_t['loc'], prior_t['scale']\n",
    "            \n",
    "            # z_sampling\n",
    "            z_t = self.reparameterize(enc_mean_t, enc_std_t)\n",
    "            \n",
    "            # decoding\n",
    "            dec_t = self.decoder(z_t, h)\n",
    "            dec_mean_t = dec_t['probs']\n",
    "            #dec_std_t = dec_t['scale']\n",
    "            \n",
    "            # recurence\n",
    "            h = self.rnn(x[t], z_t, h)['h']\n",
    "            \n",
    "            # compute loss\n",
    "            kld_loss += KLGaussianGaussian(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            \n",
    "            #nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\n",
    "            nll_loss += bi_nll(dec_mean_t, x[t])\n",
    "        return kld_loss, nll_loss\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        eps = torch.randn(mean.size()).to(device)\n",
    "        z = mean + torch.sqrt(var) * eps\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        print(_[0])\n",
    "        #print('before_squeeze: ', data.shape)\n",
    "        #print('before_squeeze: ', data[0])\n",
    "        data = data.squeeze().transpose(0, 1)\n",
    "        #print('after_squeeze: ', data.shape)\n",
    "        #print('after_squeeze: ', data[0])\n",
    "        \n",
    "        kld_loss, nll_loss, x_ts, dec_ts = vrnn(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return x_ts, dec_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pixyz\n",
    "class VRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VRNN, self).__init__()\n",
    "        self.z_prior = prior\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.rnn = recurrence\n",
    "        self.n_layers = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_dec_mean, all_dec_std = [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "        x_ts = []\n",
    "        dec_ts = []\n",
    "        \n",
    "        # torch.zeros(batch_size, h_dim), 隠れ状態の初期化\n",
    "        h = torch.zeros(x.size(1), h_dim)\n",
    "        \n",
    "        # timestep t分処理を行う(x.size(0)=行数)\n",
    "        for t in range(x.size(0)):\n",
    "            # Encoding\n",
    "            enc_t = self.encoder(x[t], h)\n",
    "            enc_mean_t, enc_std_t = enc_t['loc'], enc_t['scale']\n",
    "            \n",
    "            # prior\n",
    "            prior_t = self.z_prior(h)\n",
    "            prior_mean_t, prior_std_t = prior_t['loc'], prior_t['scale']\n",
    "            \n",
    "            # z_sampling\n",
    "            z_t = self.reparameterize(enc_mean_t, enc_std_t)\n",
    "            \n",
    "            # decoding\n",
    "            dec_t = self.decoder(z_t, h)\n",
    "            dec_mean_t = dec_t['probs']\n",
    "            #dec_std_t = dec_t['scale']\n",
    "            \n",
    "            # recurence\n",
    "            h = self.rnn(x[t], z_t, h)['h']\n",
    "            \n",
    "            # compute loss\n",
    "            kld_loss = KullbackLeibler(self.encoder, self.z_prior)\n",
    "            \n",
    "            reconst = StochasticReconstructionLoss(self.decoder, self.decoder)\n",
    "            \n",
    "            loss = (kld_loss + reconst).eval()\n",
    "            \n",
    "        return kld_loss, nll_loss\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        eps = torch.randn(mean.size()).to(device)\n",
    "        z = mean + torch.sqrt(var) * eps\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VRNN, self).__init__()\n",
    "        self.z_prior = prior\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.rnn = recurrence\n",
    "        self.n_layers = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_dec_mean, all_dec_std = [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "        x_ts = []\n",
    "        dec_ts = []\n",
    "        # Cellとの違いかわからんがhの設定が気になる\n",
    "        h = torch.zeros(x.size(1), h_dim)\n",
    "#         print('hidden_shape', h.shape)\n",
    "#         print('h', h)\n",
    "        # h[-1]ってなんぞや-> 隠れ状態の出力層に一番近いところ\n",
    "#         print('x_shape: ', x.shape)\n",
    "#         print('x[0]: ', x[0])\n",
    "        for t in range(x.size(0)):\n",
    "            print('Time step: ', t)\n",
    "            # Encoding\n",
    "#             print('x_t_shape', x[t].shape)\n",
    "#             print('x_t: ', x[t])\n",
    "#             print('h[-1]: ', h[-1])\n",
    "            enc_t = self.encoder(x[t], h)\n",
    "            enc_mean_t, enc_std_t = enc_t['loc'], enc_t['scale']\n",
    "            \n",
    "            # prior\n",
    "            prior_t = self.z_prior(h)\n",
    "            prior_mean_t, prior_std_t = prior_t['loc'], prior_t['scale']\n",
    "            \n",
    "            # z_sampling\n",
    "            z_t = self.reparameterize(enc_mean_t, enc_std_t)\n",
    "            \n",
    "            # decoding\n",
    "            dec_t = self.decoder(z_t, h)\n",
    "            dec_mean_t = dec_t['probs']\n",
    "            #dec_std_t = dec_t['scale']\n",
    "            \n",
    "            # recurence\n",
    "            h = self.rnn(x[t], z_t, h)['h'].squeeze()\n",
    "#             print('h_next shape', h.shape)\n",
    "            \n",
    "            # compute loss\n",
    "            kld_loss = self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            #nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\n",
    "            nll_loss += self._nll_bernoulli(dec_mean_t, x[t])\n",
    "            x_ts.append(x[t][0].detach().numpy())\n",
    "            dec_ts.append(dec_mean_t[0].detach().numpy())\n",
    "        return kld_loss, nll_loss, x_ts, dec_ts\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        eps = torch.randn(mean.size()).to(device)\n",
    "        z = mean + torch.sqrt(var) * eps\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrnn = VRNN()\n",
    "optimizer = torch.optim.Adam(vrnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        print(_[0])\n",
    "        #print('before_squeeze: ', data.shape)\n",
    "        #print('before_squeeze: ', data[0])\n",
    "        data = data.squeeze().transpose(0, 1)\n",
    "        #print('after_squeeze: ', data.shape)\n",
    "        #print('after_squeeze: ', data[0])\n",
    "        \n",
    "        kld_loss, nll_loss, x_ts, dec_ts = vrnn(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return x_ts, dec_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n",
      "D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b8811b492581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-42999ec81eec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print('after_squeeze: ', data[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mkld_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32941177, 0.3764706 , 0.3764706 ,\n",
       "        0.3764706 , 0.3764706 , 0.3764706 , 0.3764706 , 1.        ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.6       , 0.3764706 ,\n",
       "        0.09411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0627451 , 0.88235295, 0.9843137 , 0.99215686,\n",
       "        0.9843137 , 0.9843137 , 0.9843137 , 0.9843137 , 0.99215686,\n",
       "        0.9843137 , 0.9843137 , 0.9843137 , 0.9843137 , 0.99215686,\n",
       "        0.8039216 , 0.37254903, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.49803922, 0.9843137 , 0.9843137 , 0.99215686,\n",
       "        0.9843137 , 0.9843137 , 0.9843137 , 0.9843137 , 0.99215686,\n",
       "        0.9843137 , 0.9843137 , 0.9843137 , 0.9843137 , 0.99215686,\n",
       "        0.9843137 , 0.49411765, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0627451 , 0.88235295, 0.9843137 , 0.99215686,\n",
       "        0.8       , 0.7372549 , 0.7372549 , 0.7372549 , 0.7411765 ,\n",
       "        0.7372549 , 0.42745098, 0.27450982, 0.89411765, 0.99215686,\n",
       "        0.9843137 , 0.49411765, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3254902 , 0.36862746, 0.37254903,\n",
       "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.61960787, 0.99215686,\n",
       "        0.9843137 , 0.49411765, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.09411765, 0.7607843 , 1.        ,\n",
       "        0.8666667 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32941177, 0.9843137 , 0.99215686,\n",
       "        0.39607844, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2509804 , 0.9254902 , 0.9843137 , 0.8039216 ,\n",
       "        0.12156863, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03137255, 0.61960787, 0.9843137 , 0.9843137 , 0.46666667,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1254902 , 0.9843137 , 0.9843137 , 0.59607846, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.14117648,\n",
       "        0.8392157 , 0.99215686, 0.99215686, 0.37254903, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3764706 ,\n",
       "        0.9843137 , 0.9843137 , 0.72156864, 0.09019608, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.68235296,\n",
       "        0.9843137 , 0.7372549 , 0.05882353, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.0627451 , 0.2509804 , 0.99215686,\n",
       "        0.9843137 , 0.49411765, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2509804 , 0.9843137 , 0.99215686,\n",
       "        0.9843137 , 0.49411765, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4862745 , 0.99215686, 1.        ,\n",
       "        0.6313726 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01568628, 0.4392157 , 0.95686275, 0.9843137 , 0.70980394,\n",
       "        0.0627451 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1254902 , 0.9843137 , 0.9843137 , 0.9843137 , 0.61960787,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1254902 , 0.9843137 , 0.9843137 , 0.9843137 , 0.07450981,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1254902 , 0.9843137 , 0.827451  , 0.36862746, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1342b4e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADY1JREFUeJzt3X+s3XV9x/HXq/VS4CKDglxrW2xp\nQNMQVrabMpE5CWgqkxQSw8DEdRvjukUSScgCq1nWLLqgmQox0+VCO8qC6AwCjWlUrG4dUTpuWW35\nMfllq+1Kf1gNRWO5t33vj/utucI933M453vO91zez0dyc8/5vr8/3vnmvu73nPM553wcEQKQz6y6\nGwBQD8IPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpN/XyYCd4TpyowV4eEkjl1/qlXokjbmXd\njsJve4WkOyTNlnRXRNxWtv6JGtRFvqyTQwIosSU2tbxu2w/7bc+W9M+SPiBpqaTrbC9td38AequT\n5/zLJT0XES9ExCuSviJpZTVtAei2TsI/X9JPp9zfXSz7LbZHbI/ZHhvXkQ4OB6BKXX+1PyJGI2I4\nIoYHNKfbhwPQok7Cv0fSwin3FxTLAMwAnYT/MUnn2l5s+wRJ10raUE1bALqt7aG+iJiwfaOkb2ly\nqG9dRDxZWWcAuqqjcf6I2ChpY0W9AOgh3t4LJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k\nRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIP\nJEX4gaQIP5AU4QeSIvxAUh3N0mt7p6TDko5KmoiI4SqaAtB9HYW/cGlEHKxgPwB6iIf9QFKdhj8k\nfdv2VtsjVTQEoDc6fdh/SUTssX2WpIdt/29EbJ66QvFPYUSSTtTJHR4OQFU6uvJHxJ7i935JD0ha\nPs06oxExHBHDA5rTyeEAVKjt8NsetP3m47clvV/SE1U1BqC7OnnYPyTpAdvH9/PliPhmJV0B6Lq2\nwx8RL0j63Qp7AdBDDPUBSRF+ICnCDyRF+IGkCD+QFOEHkqriU33p/eyGd5XWx09xjzqp3g//5oul\n9fE42rVjL/3P60vrJz9W/nbxBQ/uaVib+PGutnp6I+HKDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJ\nOSJ6drBTPTcu8mU9O16VZg+d1bD2V488Urrt5Sf9orQ+4Nml9W6OpTczk3v75MELGtYe/Nc/Kt32\nrbd/v62e6rYlNumlONTSG0u48gNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUnyev0U/uuWchrXLT/pG\nDztBq245438a1u5d+Ic97KQ/ceUHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaSajvPbXifpg5L2R8T5\nxbK5kr4qaZGknZKuiYifd6/N+r3j0y80rH3nytNKt232ef5m/vHg75fWn/vlWzraf5lZKv++h2Mq\n/+j49m++s2HttGeOlW675lNrS+uXnvTr0jrKtXLlv1vSilctu1XSpog4V9Km4j6AGaRp+CNis6RD\nr1q8UtL64vZ6SVdV3BeALmv3Of9QROwtbr8oaaiifgD0SMcv+MXklwA2fGJoe8T2mO2xcR3p9HAA\nKtJu+PfZnidJxe/9jVaMiNGIGI6I4QHNafNwAKrWbvg3SFpV3F4l6aFq2gHQK03Db/s+ST+Q9A7b\nu21fL+k2Se+z/ayky4v7AGaQpuP8EXFdg9LM/AL+Nh3d1/CZjf7h06sa1iTp7wabfI16k/KCB3eX\n1id2/qR8BzVaqPa///7jH7q2tL794rvb3jd4hx+QFuEHkiL8QFKEH0iK8ANJEX4gKb66uwJn3PWD\nru5/oqt7r8+bFr+9tH7J2Y0/Ri01n6L7/pfPbFhbcvOjpdtmwJUfSIrwA0kRfiApwg8kRfiBpAg/\nkBThB5JinB+12Xf520rrX5v/tdL6eJSP839qbaNPo0tv6+Cjxm8UXPmBpAg/kBThB5Ii/EBShB9I\nivADSRF+ICnG+dFVv7r6ooa1u1bf3mTr8u80/+TBC0rrC79xsGHtaJMjZ8CVH0iK8ANJEX4gKcIP\nJEX4gaQIP5AU4QeSajrOb3udpA9K2h8R5xfL1ki6QdKBYrXVEbGxW02if80aHCytH/zwrxrWzhto\nMjd5E/dtfE9pffFT3Z1PYaZr5cp/t6QV0yz/fEQsK34IPjDDNA1/RGyWdKgHvQDooU6e899oe7vt\ndbZPr6wjAD3Rbvi/JGmJpGWS9kr6bKMVbY/YHrM9Nq4jbR4OQNXaCn9E7IuIoxFxTNKdkpaXrDsa\nEcMRMTygOe32CaBibYXf9rwpd6+W9EQ17QDolVaG+u6T9F5JZ9reLenvJb3X9jJJIWmnpI92sUcA\nXdA0/BEx3Zefr+1CL5iBvGhBaX3ru9r/U7lg80hpfcnfMo7fCd7hByRF+IGkCD+QFOEHkiL8QFKE\nH0iKr+5GqVnLlpbW//jL/1VaH3D5NNplBv/75La3RXNc+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4g\nKcb5Uer/Lj2ttP7npz5fWh+PxrUPP39l6bYLHtxdWp8oraIZrvxAUoQfSIrwA0kRfiApwg8kRfiB\npAg/kBTj/Cj1Oyv2drT9X+yaboLnSa986FjptkcP/KSjY6McV34gKcIPJEX4gaQIP5AU4QeSIvxA\nUoQfSKrpOL/thZLukTQkKSSNRsQdtudK+qqkRZJ2SromIn7evVbRjtlDZ5XWD1+8uLT+3fO/WFov\n+7y+JI09el7D2pIDj5ZvjK5q5co/IenmiFgq6Q8kfcz2Ukm3StoUEedK2lTcBzBDNA1/ROyNiMeL\n24clPS1pvqSVktYXq62XdFW3mgRQvdf1nN/2IkkXStoiaSgijr/380VNPi0AMEO0HH7bp0i6X9JN\nEfHS1FpEhCZfD5huuxHbY7bHxnWko2YBVKel8Nse0GTw742IrxeL99meV9TnSdo/3bYRMRoRwxEx\nPKA5VfQMoAJNw2/bktZKejoiPjeltEHSquL2KkkPVd8egG5p5SO975b0EUk7bG8rlq2WdJukf7d9\nvaRdkq7pTovoRLOhvG994QtN9tD+FNvob03DHxGPSHKD8mXVtgOgV3iHH5AU4QeSIvxAUoQfSIrw\nA0kRfiApvrr7DW7vxeX/3wdcPo7frH7hlj8trS+5mY/t9iuu/EBShB9IivADSRF+ICnCDyRF+IGk\nCD+QFOP8M4AHTiit7/rEcMPaf/zJZ0q3HY/yfa985srS+tl/faC0frS0ijpx5QeSIvxAUoQfSIrw\nA0kRfiApwg8kRfiBpBjnnwFmnVf+3ftb//L2kmr5OH4zPz54Rmn97H07Oto/6sOVH0iK8ANJEX4g\nKcIPJEX4gaQIP5AU4QeSajrOb3uhpHskDUkKSaMRcYftNZJukHT8A92rI2JjtxpFPc656Wel9Yke\n9YHqtfImnwlJN0fE47bfLGmr7YeL2ucj4p+61x6Abmka/ojYK2lvcfuw7aclze92YwC663U957e9\nSNKFkrYUi260vd32OtunN9hmxPaY7bFxHemoWQDVaTn8tk+RdL+kmyLiJUlfkrRE0jJNPjL47HTb\nRcRoRAxHxPCA5lTQMoAqtBR+2wOaDP69EfF1SYqIfRFxNCKOSbpT0vLutQmgak3Db9uS1kp6OiI+\nN2X5vCmrXS3pierbA9Atrbza/25JH5G0w/a2YtlqSdfZXqbJ4b+dkj7alQ7RkX/5xTtL6+vvXFFa\nf+ue71fZDvpIK6/2PyLJ05QY0wdmMN7hByRF+IGkCD+QFOEHkiL8QFKEH0jKEdGzg53quXGRL+vZ\n8YBstsQmvRSHphuafw2u/EBShB9IivADSRF+ICnCDyRF+IGkCD+QVE/H+W0fkLRryqIzJR3sWQOv\nT7/21q99SfTWrip7e3tEvKWVFXsa/tcc3B6LiOHaGijRr731a18SvbWrrt542A8kRfiBpOoO/2jN\nxy/Tr731a18SvbWrlt5qfc4PoD51X/kB1KSW8NteYftHtp+zfWsdPTRie6ftHba32R6ruZd1tvfb\nfmLKsrm2H7b9bPF72mnSauptje09xbnbZvuKmnpbaPt7tp+y/aTtjxfLaz13JX3Vct56/rDf9mxJ\nz0h6n6Tdkh6TdF1EPNXTRhqwvVPScETUPiZs+z2SXpZ0T0ScXyz7jKRDEXFb8Y/z9Ii4pU96WyPp\n5bpnbi4mlJk3dWZpSVdJ+jPVeO5K+rpGNZy3Oq78yyU9FxEvRMQrkr4iaWUNffS9iNgs6dCrFq+U\ntL64vV6Tfzw916C3vhAReyPi8eL2YUnHZ5au9dyV9FWLOsI/X9JPp9zfrf6a8jskfdv2VtsjdTcz\njaFi2nRJelHSUJ3NTKPpzM299KqZpfvm3LUz43XVeMHvtS6JiN+T9AFJHyse3valmHzO1k/DNS3N\n3Nwr08ws/Rt1nrt2Z7yuWh3h3yNp4ZT7C4plfSEi9hS/90t6QP03+/C+45OkFr/319zPb/TTzM3T\nzSytPjh3/TTjdR3hf0zSubYX2z5B0rWSNtTQx2vYHixeiJHtQUnvV//NPrxB0qri9ipJD9XYy2/p\nl5mbG80srZrPXd/NeB0RPf+RdIUmX/F/XtIn6uihQV/nSPph8fNk3b1Juk+TDwPHNfnayPWSzpC0\nSdKzkr4jaW4f9fZvknZI2q7JoM2rqbdLNPmQfrukbcXPFXWfu5K+ajlvvMMPSIoX/ICkCD+QFOEH\nkiL8QFKEH0iK8ANJEX4gKcIPJPX/BusSoiSurWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_t = np.array(x_ts)\n",
    "x_t = x_t.reshape(28, 28)\n",
    "print(x_t.shape)\n",
    "plt.imshow(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1341846a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxLft5kVDGwEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dec_t = np.array(dec_ts)\n",
    "dec_t = dec_t.reshape(28, 28)\n",
    "print(dec_t.shape)\n",
    "plt.imshow(dec_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions (for training): \n",
      "  q(z|x,h_{prev}), p(x|z,h_{prev}), p(z|h_{prev}), p(h|x,z,h_{prev}) \n",
      "Loss function: \n",
      "  \\sum_{t=1}^{28} mean \\left(D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right] - \\mathbb{E}_{p(h,z|x,h_{prev})} \\left[\\log p(x|z,h_{prev}) \\right] \\right) \n",
      "Optimizer: \n",
      "  Adam (\n",
      "  Parameter Group 0\n",
      "      amsgrad: False\n",
      "      betas: (0.9, 0.999)\n",
      "      eps: 1e-08\n",
      "      lr: 0.001\n",
      "      weight_decay: 0\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\sum_{t=1}^{28} mean \\left(D_{KL} \\left[q(z|x,h_{prev})||p(z|h_{prev}) \\right] - \\mathbb{E}_{p(h,z|x,h_{prev})} \\left[\\log p(x|z,h_{prev}) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconst = StochasticReconstructionLoss(encoder_with_recurrence, decoder)\n",
    "kl = KullbackLeibler(encoder, prior)\n",
    "\n",
    "step_loss = (reconst + kl).mean()\n",
    "loss = IterativeLoss(step_loss, max_iter=t_max,\n",
    "                     series_var=['x'],\n",
    "                     update_value={\"h\": \"h_prev\"})\n",
    "\n",
    "vrnn = Model(loss, distributions=[encoder, decoder, prior, recurrence],\n",
    "             optimizer=optim.Adam, optimizer_params={'lr': 1e-3})\n",
    "\n",
    "print(vrnn)\n",
    "print_latex(vrnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_loop(epoch, loader, model, device, train_mode=False):\n",
    "    mean_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(tqdm(loader)):\n",
    "        data = data.to(device)\n",
    "        batch_size = data.size()[0]\n",
    "        x = data.transpose(0, 1)\n",
    "        h_prev = torch.zeros(batch_size, recurrence.hidden_size).to(device)\n",
    "        if train_mode:\n",
    "            mean_loss += model.train({'x': x, 'h_prev': h_prev}).item() * batch_size\n",
    "        else:\n",
    "            mean_loss += model.test({'x': x, 'h_prev': h_prev}).item() * batch_size\n",
    "\n",
    "    mean_loss /= len(loader.dataset)\n",
    "    if train_mode:\n",
    "        print('Epoch: {} Train loss: {:.4f}'.format(epoch, mean_loss))\n",
    "    else:\n",
    "        print('Test loss: {:.4f}'.format(mean_loss))\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image_from_latent(batch_size):\n",
    "    x = []\n",
    "    h_prev = torch.zeros(batch_size, recurrence.hidden_size).to(device)\n",
    "    for step in range(t_max):\n",
    "        samples = generate_from_prior.sample({'h_prev': h_prev})\n",
    "        x_t = decoder.sample_mean({\"z\": samples[\"z\"], \"h_prev\": samples[\"h_prev\"]})\n",
    "        h_prev = samples[\"h\"]\n",
    "        x.append(x_t[None, :])\n",
    "    x = torch.cat(x, dim=0).transpose(0, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [07:02<00:00,  4.47it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 213.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:28<00:00, 12.50it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 196.9736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [07:18<00:00,  5.03it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 189.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:27<00:00, 12.11it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 185.0552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [07:02<00:00,  5.01it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train loss: 184.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:26<00:00, 11.61it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 180.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:58<00:00,  5.05it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train loss: 178.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:34<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 176.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [11:26:32<00:00,  4.10it/s]       \n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train loss: 176.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:49<00:00, 10.51it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 175.5571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [33:48<00:00,  5.19it/s]    \n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train loss: 176.1811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:27<00:00, 11.40it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 175.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [07:16<00:00,  4.21it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train loss: 175.8236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:32<00:00, 10.84it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 174.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:38<00:00,  4.94it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train loss: 175.5170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:36<00:00, 11.12it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 174.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [07:30<00:00,  4.65it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train loss: 175.3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:35<00:00, 11.55it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 174.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [06:36<00:00,  4.90it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train loss: 175.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:28<00:00, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 174.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = data_loop(epoch, train_loader, vrnn, device, train_mode=True)\n",
    "    test_loss = data_loop(epoch, test_loader, vrnn, device)\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "\n",
    "    sample = plot_image_from_latent(batch_size)[:, None]\n",
    "    writer.add_images('Image_from_latent', sample, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
